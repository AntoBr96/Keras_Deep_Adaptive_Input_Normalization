{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras/Tensorflow implementation of the Deep Adaptive Input Normalization layer for Time Series Forecasting\n",
    "\n",
    "This notebook contains the Keras/Tensorflow Layer implementation of the Deep Adaptive Input Normalization model  for Time Series Forecasting proposed by Passalis *et al.* ([Deep Adaptive Input Normalization for Time series Forecasting](https://arxiv.org/pdf/1902.07892.pdf)).\n",
    "\n",
    "The authors of the above mentioned paper propose a PyTorch implementation ([PyTorch implementation](https://github.com/passalis/dain)) of the model. A slightly reviewed version (software structure) is here reported. Results obtained by the two implementations are compared through an explicative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras/Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_Normalizer_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, mode = 'full', input_dim = 2):\n",
    "        super(Adaptive_Normalizer_Layer, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        PARAMETERS\n",
    "        \n",
    "        :param mode: Type of normalization to be performed.\n",
    "                        - 'adaptive_average' performs the adaptive average of the inputs\n",
    "                        - 'adaptive_scale' performs the adaptive z-score normalization of the inputs\n",
    "                        - 'full' (Default) performs the complete normalization process: adaptive_average + adaptive_scale + gating\n",
    "        :param input_dim\n",
    "        '''\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.x = None\n",
    "\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "        initializer = tf.keras.initializers.Identity()\n",
    "        gate_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        self.linear_1 = tf.keras.layers.Dense(input_dim, kernel_initializer=initializer, use_bias=False)\n",
    "        self.linear_2 = tf.keras.layers.Dense(input_dim, kernel_initializer=initializer, use_bias=False)\n",
    "        self.linear_3 = tf.keras.layers.Dense(input_dim, kernel_initializer=gate_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expecting (n_samples, dim, n_feature_vectors)\n",
    "        \n",
    "        def adaptive_avg(inputs):\n",
    "        \n",
    "            avg = tf.keras.backend.mean(inputs, 2)\n",
    "            adaptive_avg = self.linear_1(avg)\n",
    "            adaptive_avg = tf.keras.backend.reshape(adaptive_avg, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = inputs - adaptive_avg\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def adaptive_std(x):\n",
    "        \n",
    "            std = tf.keras.backend.mean(x ** 2, 2)\n",
    "            std = tf.keras.backend.sqrt(std + self.eps)\n",
    "            adaptive_std = self.linear_2(std)\n",
    "            adaptive_std = tf.where(tf.math.less_equal(adaptive_std, self.eps), 1, adaptive_std)\n",
    "            adaptive_std = tf.keras.backend.reshape(adaptive_std, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = x / (adaptive_std)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def gating(x):\n",
    "            \n",
    "            gate = tf.keras.backend.mean(x, 2)\n",
    "            gate = self.linear_3(gate)\n",
    "            gate = tf.math.sigmoid(gate)\n",
    "            gate = tf.keras.backend.reshape(gate, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = x * gate\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        if self.mode == None:\n",
    "            pass\n",
    "        \n",
    "        elif self.mode == 'adaptive_average':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            \n",
    "        elif self.mode == 'adaptive_scale':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            self.x = adaptive_std(x)\n",
    "            \n",
    "        elif self.mode == 'full':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            self.x = adaptive_std(self.x)\n",
    "            self.x = gating(self.x)\n",
    "        \n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now propose a shallow experiment to test the *Adaptive_Normalizer_Layer*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = tf.constant([\n",
    "  [[0.0, 1.0, 2.0, 3.0, 4.0],\n",
    "   [5.0, 6.0, 7.0, 8.0, 9.0]],\n",
    "  [[10.0, 11.0, 12.0, 13.0, 14.0],\n",
    "   [15.0, 16.0, 17.0, 18.0, 19.0]],\n",
    "  [[20.0, 21.0, 22.0, 23.0, 24.0],\n",
    "   [25.0, 26.0, 27.0, 28.0, 29.0]],])\n",
    "\n",
    "keras_layer = Adaptive_Normalizer_Layer(input_dim = tf.shape(example_tensor)[1].numpy())\n",
    "output = keras_layer(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=370, shape=(3, 2, 5), dtype=float32, numpy=\n",
       "array([[[-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677],\n",
       "        [-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677]],\n",
       "\n",
       "       [[-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677],\n",
       "        [-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677]],\n",
       "\n",
       "       [[-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677],\n",
       "        [-0.70710677, -0.35355338,  0.        ,  0.35355338,\n",
       "          0.70710677]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAIN_Layer(nn.Module):\n",
    "    def __init__(self, mode='full', mean_lr=0.00001, gate_lr=0.001, scale_lr=0.00001, input_dim=2):\n",
    "        super(DAIN_Layer, self).__init__()\n",
    "        \n",
    "        #print('Mode = ', mode)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.mean_lr = mean_lr\n",
    "        self.gate_lr = gate_lr\n",
    "        self.scale_lr = scale_lr\n",
    "        \n",
    "        # Parameters for adaptive average\n",
    "        self.mean_layer = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.mean_layer.weight.data = torch.FloatTensor(data = np.eye(input_dim, input_dim))\n",
    "        \n",
    "        # Parameters for adaptive std\n",
    "        self.scaling_layer = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.scaling_layer.weight.data = torch.FloatTensor(data = np.eye(input_dim, input_dim))\n",
    "        \n",
    "        # Parameters for adaptive scaling\n",
    "        self.gating_layer = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Expecting (n_samples, dim, n_feature_vectors)\n",
    "\n",
    "        def adaptive_avg(x):\n",
    "            avg = torch.mean(x, 2)\n",
    "            adaptive_avg = self.mean_layer(avg)\n",
    "            adaptive_avg = adaptive_avg.resize(adaptive_avg.size(0), adaptive_avg.size(1), 1)\n",
    "            x = x - adaptive_avg\n",
    "            return x\n",
    "\n",
    "        def adaptive_std(x):\n",
    "            std = torch.mean(x ** 2, 2)\n",
    "            std = torch.sqrt(std + self.eps)\n",
    "            adaptive_std = self.scaling_layer(std)\n",
    "            adaptive_std[adaptive_std <= self.eps] = 1\n",
    "            \n",
    "            adaptive_std = adaptive_std.resize(adaptive_std.size(0), adaptive_std.size(1), 1)\n",
    "            x = x / (adaptive_std)\n",
    "            return x\n",
    "\n",
    "        def gating(x):\n",
    "            avg = torch.mean(x,2)\n",
    "            avg = self.gating_layer(avg)\n",
    "            gate = F.sigmoid(avg)\n",
    "            gate = gate.resize(gate.size(0), gate.size(1), 1)\n",
    "            x = x * gate\n",
    "            return x\n",
    "        \n",
    "        # Nothing to normalize\n",
    "        if self.mode == None:\n",
    "            pass\n",
    "        \n",
    "        # Do simple average normalization\n",
    "        elif self.mode == 'avg':\n",
    "            avg = avg.resize(avg.size(0), avg.size(1), 1)\n",
    "            x = x - avg\n",
    "        \n",
    "        # Perform only the adaptive averaging step\n",
    "        elif self.mode == 'adaptive_avg':\n",
    "            x = adaptive_avg(x)\n",
    "            \n",
    "        # Perform the adaptive averaging + adaptive scaling\n",
    "        elif self.mode == 'adaptive_scale':\n",
    "            \n",
    "            # Step 1\n",
    "            x = adaptive_avg(x)\n",
    "            # Step 2\n",
    "            x = adaptive_std(x)\n",
    "        \n",
    "        # Perform the adaptive averaging + adaptive scaling + gating\n",
    "        elif self.mode == 'full':\n",
    "            \n",
    "            # Step 1:\n",
    "            x = adaptive_avg(x)\n",
    "            # Step 2:\n",
    "            x = adaptive_std(x)\n",
    "            # Step 3\n",
    "            x = gating(x)\n",
    "            \n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now propose a shallow experiment to test the *DAIN_Layer* and to compare obtained results with the ones achieved by the *Adaptive_Normalizer_Layer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py:365: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "example_tensor = torch.tensor([\n",
    "  [[0.0, 1.0, 2.0, 3.0, 4.0],\n",
    "   [5.0, 6.0, 7.0, 8.0, 9.0]],\n",
    "  [[10.0, 11.0, 12.0, 13.0, 14.0],\n",
    "   [15.0, 16.0, 17.0, 18.0, 19.0]],\n",
    "  [[20.0, 21.0, 22.0, 23.0, 24.0],\n",
    "   [25.0, 26.0, 27.0, 28.0, 29.0]],])\n",
    "\n",
    "torch_layer = DAIN_Layer()\n",
    "output = torch_layer(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5047, -0.2523,  0.0000,  0.2523,  0.5047],\n",
       "         [-0.4686, -0.2343,  0.0000,  0.2343,  0.4686]],\n",
       "\n",
       "        [[-0.5047, -0.2523,  0.0000,  0.2523,  0.5047],\n",
       "         [-0.4686, -0.2343,  0.0000,  0.2343,  0.4686]],\n",
       "\n",
       "        [[-0.5047, -0.2523,  0.0000,  0.2523,  0.5047],\n",
       "         [-0.4686, -0.2343,  0.0000,  0.2343,  0.4686]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "It is possible to note how the two implementations achieve very similar results. The differences are justified by the initialization parameters adopted in the Dense/Layer gating layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The author of this notebook just implemented the Keras/Tensorflow version of a model originally defined in Passalis *et al.* ([Deep Adaptive Input Normalization for Time series Forecasting](https://arxiv.org/pdf/1902.07892.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
