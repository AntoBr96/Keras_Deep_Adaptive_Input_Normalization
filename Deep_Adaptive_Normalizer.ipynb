{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras/Tensorflow implementation of the Deep Adaptive Input Normalization layer for Time Series Forecasting\n",
    "\n",
    "This notebook contains the Keras/Tensorflow Layer implementation of the Deep Adaptive Input Normalization model  for Time Series Forecasting proposed by Passalis *et al.* ([Deep Adaptive Input Normalization for Time series Forecasting](https://arxiv.org/pdf/1902.07892.pdf)).\n",
    "\n",
    "The authors of the above mentioned paper propose a PyTorch implementation ([PyTorch implementation](https://github.com/passalis/dain)) of the model. A slightly reviewed version (software structure) is here reported. Results obtained by the two implementations are compared through an explicative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras/Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_Normalizer_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, mode = 'full', input_dim = 5):\n",
    "        super(Adaptive_Normalizer_Layer, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        PARAMETERS\n",
    "        \n",
    "        :param mode: Type of normalization to be performed.\n",
    "                        - 'adaptive_average' performs the adaptive average of the inputs\n",
    "                        - 'adaptive_scale' performs the adaptive z-score normalization of the inputs\n",
    "                        - 'full' (Default) performs the complete normalization process: adaptive_average + adaptive_scale + gating\n",
    "        :param input_dim: Number of rows in each batch\n",
    "        '''\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.x = None\n",
    "\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "        initializer = tf.keras.initializers.Identity()\n",
    "        gate_initializer =  tf.keras.initializers.GlorotNormal()\n",
    "        bias_initializer = tf.keras.initializers.RandomNormal()\n",
    "        self.linear_1 = tf.keras.layers.Dense(input_dim, kernel_initializer=initializer, use_bias=False)\n",
    "        self.linear_2 = tf.keras.layers.Dense(input_dim, kernel_initializer=initializer, use_bias=False)\n",
    "        self.linear_3 = tf.keras.layers.Dense(input_dim, kernel_initializer=gate_initializer, bias_initializer=gate_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expecting (n_samples, dim, n_feature_vectors)\n",
    "        \n",
    "        def adaptive_avg(inputs):\n",
    "        \n",
    "            avg = tf.keras.backend.mean(inputs, 2)\n",
    "            adaptive_avg = self.linear_1(avg)\n",
    "            adaptive_avg = tf.keras.backend.reshape(adaptive_avg, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = inputs - adaptive_avg\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def adaptive_std(x):\n",
    "        \n",
    "            std = tf.keras.backend.mean(x ** 2, 2)\n",
    "            std = tf.keras.backend.sqrt(std + self.eps)\n",
    "            adaptive_std = self.linear_2(std)\n",
    "            adaptive_std = tf.where(tf.math.less_equal(adaptive_std, self.eps), 1, adaptive_std)\n",
    "            adaptive_std = tf.keras.backend.reshape(adaptive_std, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = x / (adaptive_std)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def gating(x):\n",
    "            \n",
    "            gate = tf.keras.backend.mean(x, 2)\n",
    "            gate = self.linear_3(gate)\n",
    "            gate = tf.math.sigmoid(gate)\n",
    "            gate = tf.keras.backend.reshape(gate, (tf.shape(inputs)[0].numpy(), tf.shape(inputs)[1].numpy(), 1))\n",
    "            x = x * gate\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        if self.mode == None:\n",
    "            pass\n",
    "        \n",
    "        elif self.mode == 'adaptive_average':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            \n",
    "        elif self.mode == 'adaptive_scale':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            self.x = adaptive_std(x)\n",
    "            \n",
    "        elif self.mode == 'full':\n",
    "            self.x = adaptive_avg(inputs)\n",
    "            self.x = adaptive_std(self.x)\n",
    "            self.x = gating(self.x)\n",
    "        \n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now propose a shallow experiment to test the *Adaptive_Normalizer_Layer*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = tf.constant([\n",
    "  [[0.0, 1.0, 2.0, 3.0, 4.0],\n",
    "   [5.0, 6.0, 7.0, 8.0, 9.0]],\n",
    "  [[10.0, 11.0, 12.0, 13.0, 14.0],\n",
    "   [15.0, 16.0, 17.0, 18.0, 19.0]],\n",
    "  [[20.0, 21.0, 22.0, 23.0, 24.0],\n",
    "   [25.0, 26.0, 27.0, 28.0, 29.0]],], dtype=np.float64)\n",
    "\n",
    "keras_layer = Adaptive_Normalizer_Layer()\n",
    "example_tensor = tf.transpose(example_tensor, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras_layer(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2926, shape=(3, 2, 5), dtype=float64, numpy=\n",
       "array([[[-0.3957139 , -0.50543095, -0.60220335, -0.70781163,\n",
       "         -0.3916883 ],\n",
       "        [ 0.3957139 ,  0.50543095,  0.60220335,  0.70781163,\n",
       "          0.3916883 ]],\n",
       "\n",
       "       [[-0.3957139 , -0.50543095, -0.60220335, -0.70781163,\n",
       "         -0.3916883 ],\n",
       "        [ 0.3957139 ,  0.50543095,  0.60220335,  0.70781163,\n",
       "          0.3916883 ]],\n",
       "\n",
       "       [[-0.3957139 , -0.50543095, -0.60220335, -0.70781163,\n",
       "         -0.3916883 ],\n",
       "        [ 0.3957139 ,  0.50543095,  0.60220335,  0.70781163,\n",
       "          0.3916883 ]]])>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.transpose(output, perm=[0, 2, 1])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAIN_Layer(nn.Module):\n",
    "    def __init__(self, mode='full', mean_lr=0.00001, gate_lr=0.001, scale_lr=0.00001, input_dim=5):\n",
    "        super(DAIN_Layer, self).__init__()\n",
    "        \n",
    "        #print('Mode = ', mode)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.mean_lr = mean_lr\n",
    "        self.gate_lr = gate_lr\n",
    "        self.scale_lr = scale_lr\n",
    "        \n",
    "        # Parameters for adaptive average\n",
    "        self.mean_layer = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.mean_layer.weight.data = torch.FloatTensor(data = np.eye(input_dim, input_dim))\n",
    "        \n",
    "        # Parameters for adaptive std\n",
    "        self.scaling_layer = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.scaling_layer.weight.data = torch.FloatTensor(data = np.eye(input_dim, input_dim))\n",
    "        \n",
    "        # Parameters for adaptive scaling\n",
    "        self.gating_layer = nn.Linear(input_dim, input_dim)\n",
    "        #self.gating_layer.weight.data = torch.FloatTensor(data = np.eye(input_dim, input_dim))\n",
    "        \n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Expecting (window_length, batch_size, n_features)\n",
    "        # [batch_size, rows, columns]\n",
    "\n",
    "        def adaptive_avg(x):\n",
    "            avg = torch.mean(x, 2)\n",
    "            print(avg)\n",
    "            print(avg.shape)\n",
    "            adaptive_avg = self.mean_layer(avg)\n",
    "            adaptive_avg = adaptive_avg.resize(adaptive_avg.size(0), adaptive_avg.size(1), 1)\n",
    "            x = x - adaptive_avg\n",
    "            return x\n",
    "\n",
    "        def adaptive_std(x):\n",
    "            std = torch.mean(x ** 2, 2)\n",
    "            std = torch.sqrt(std + self.eps)\n",
    "            adaptive_std = self.scaling_layer(std)\n",
    "            adaptive_std[adaptive_std <= self.eps] = 1\n",
    "            \n",
    "            adaptive_std = adaptive_std.resize(adaptive_std.size(0), adaptive_std.size(1), 1)\n",
    "            x = x / (adaptive_std)\n",
    "            return x\n",
    "\n",
    "        def gating(x):\n",
    "            avg = torch.mean(x,2)\n",
    "            print(avg)\n",
    "            avg = self.gating_layer(avg)\n",
    "            print(avg)\n",
    "            gate = F.sigmoid(avg)\n",
    "            gate = gate.resize(gate.size(0), gate.size(1), 1)\n",
    "            x = x * gate\n",
    "            return x\n",
    "        \n",
    "        # Nothing to normalize\n",
    "        if self.mode == None:\n",
    "            pass\n",
    "        \n",
    "        # Do simple average normalization\n",
    "        elif self.mode == 'avg':\n",
    "            avg = avg.resize(avg.size(0), avg.size(1), 1)\n",
    "            x = x - avg\n",
    "        \n",
    "        # Perform only the adaptive averaging step\n",
    "        elif self.mode == 'adaptive_avg':\n",
    "            x = adaptive_avg(x)\n",
    "            \n",
    "        # Perform the adaptive averaging + adaptive scaling\n",
    "        elif self.mode == 'adaptive_scale':\n",
    "            \n",
    "            # Step 1\n",
    "            x = adaptive_avg(x)\n",
    "            # Step 2\n",
    "            x = adaptive_std(x)\n",
    "        \n",
    "        # Perform the adaptive averaging + adaptive scaling + gating\n",
    "        elif self.mode == 'full':\n",
    "            \n",
    "            # Step 1:\n",
    "            x = adaptive_avg(x)\n",
    "            # Step 2:\n",
    "            x = adaptive_std(x)\n",
    "            # Step 3\n",
    "            x = gating(x)\n",
    "            \n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now propose a shallow experiment to test the *DAIN_Layer* and to compare obtained results with the ones achieved by the *Adaptive_Normalizer_Layer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 5])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor = torch.tensor([\n",
    "  [[0.0, 1.0, 2.0, 3.0, 4.0],\n",
    "   [5.0, 6.0, 7.0, 8.0, 9.0]],\n",
    "  [[10.0, 11.0, 12.0, 13.0, 14.0],\n",
    "   [15.0, 16.0, 17.0, 18.0, 19.0]],\n",
    "  [[20.0, 21.0, 22.0, 23.0, 24.0],\n",
    "   [25.0, 26.0, 27.0, 28.0, 29.0]],])\n",
    "\n",
    "torch_layer = DAIN_Layer(mode='full')\n",
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 5])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5000,  3.5000,  4.5000,  5.5000,  6.5000],\n",
      "        [12.5000, 13.5000, 14.5000, 15.5000, 16.5000],\n",
      "        [22.5000, 23.5000, 24.5000, 25.5000, 26.5000]])\n",
      "torch.Size([3, 5])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "tensor([[-0.4400,  0.0863,  0.1173, -0.2174, -0.0367],\n",
      "        [-0.4400,  0.0863,  0.1173, -0.2174, -0.0367],\n",
      "        [-0.4400,  0.0863,  0.1173, -0.2174, -0.0367]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "example_tensor = example_tensor.transpose(1, 2)\n",
    "output = torch_layer(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3917, -0.5216, -0.5293, -0.4459, -0.4908],\n",
       "         [ 0.3917,  0.5216,  0.5293,  0.4459,  0.4908]],\n",
       "\n",
       "        [[-0.3917, -0.5216, -0.5293, -0.4459, -0.4908],\n",
       "         [ 0.3917,  0.5216,  0.5293,  0.4459,  0.4908]],\n",
       "\n",
       "        [[-0.3917, -0.5216, -0.5293, -0.4459, -0.4908],\n",
       "         [ 0.3917,  0.5216,  0.5293,  0.4459,  0.4908]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "It is possible to note how the two implementations achieve very similar results. The differences are justified by the initialization parameters adopted in the Dense/Linear gating layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The author of this notebook just implemented the Keras/Tensorflow version of a model originally defined in Passalis *et al.* ([Deep Adaptive Input Normalization for Time series Forecasting](https://arxiv.org/pdf/1902.07892.pdf))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
